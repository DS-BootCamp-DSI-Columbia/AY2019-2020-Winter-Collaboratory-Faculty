---
title: "Stan Language Lab"
author: "Ben Goodrich"
editor_options:
  chunk_output_type: console
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{color}
output: 
  html_document: 
    number_sections: yes
---

This morning we considered a contrived situation where the probabilities of a bowler knocking down
$0, 1, \dots, 10$ pins were given by a _known_ function involving ratios of Fibonacci numbers. In
this lab, we consider a more realistic situation where we want to _estimate_ the probabilities of a 
professional bowler knocking down $0, 1, \dots, 10$ pins given frames of data from the $2010$ US Open 
bowling tournament.

```{r, message = FALSE}
# reformated data originally assembled at 
# https://www.cs.rpi.edu/academics/courses/fall14/csci1200/hw/02_bowling_classes/2010_US_Open.txt
US_Open2010 <- readr::read_csv("US_Open2010.csv")
str(US_Open2010)
with(US_Open2010, table(roll_1, roll_2))
```

For simplicity, data on extra rolls awarded to a bowler in the tenth frame are omitted from this
dataset.

# Simplest Model

To start with, we will consider the simplest setup where all bowlers in the $2010$ US Open
are essentially indistiguishable and therefore pool all their frames of data together.

The categorical distribution is used to model which element of a finite set is realized
and has a simple PMF

$$\Pr\left(x \mid \boldsymbol{\pi}\right) = \pi_x$$
where $\boldsymbol{\pi}$ is a simplex vector whose size $\left(K\right)$ is equal to the number of elements in the 
sample space for $X$, which in this case is $11$. A simplex vector is non-negative and sums to $1$, so its elements
are probabilities. Considered as a function of $\boldsymbol{\pi}$, $\ln \pi_x$ can be used
as a contribution to the log-likelihood in a Bayesian model, and the log-likelihood contributions from all rolls 
under consideration need to be added together.

The most common prior distribution for an unknown simplex vector is the Dirichlet distribution, which is
a generalization of the Beta distribution to the case of more than two possibilities and has the PDF

$$f\left(\boldsymbol{\pi} \mid \mathbf{a}\right) = \frac{\Gamma\left(\sum_{k = 1}^K a_k\right)}{\prod_{k = 1}^K \Gamma\left(a_k\right)} \prod_{k = 1}^K \pi_k^{a_k - 1}$$
where $\Gamma\left(a\right) = \int_0^\infty x^{a - 1} e^{-x} dx$ is the gamma function, whose logarithm is 
denoted by `lgamma` in R and Stan, and $\mathbf{a}$ is a positive vector of size $K$ that governs the shape 
of the Dirichlet distribution. The expectation of the $k$-th element of a Dirichlet-distributed random variable is
$$\forall k \ \mathbb{E} \pi_k = \frac{a_k}{\sum_{j = 1}^K a_j}.$$
and the larger is the sum in the denominator, the smaller is the variance.

## Simplest `model` Block

Create a text file called simplest.stan and paste the following `data` and `parameters` blocks into it. To
make indexing easier in Stan, this `data` block assumes that the data on two rolls of a frame of bowling are 
indexed from $1$ (a gutter ball) to $11$ (a strike) rather than $0$ to $10$, but this choice has no bearing 
on the posterior distribution.

```{stan output.var="data_block", eval = FALSE}
data {
  int<lower = 0> N;                     // number of frames with data
  int<lower = 1, upper = 11> roll_1[N]; // 1 = gutter ball, ..., 11 = strike
  int<lower = 1, upper = 11> roll_2[N]; // same
  vector<lower = 0>[11] a;              // hyperparameters for Dirichlet prior
}
parameters {
  simplex[11] pi;                       // probability of 1 ... 11
}
```

Below that, write a `model` block that increments the `target` keyword with the logarithm of a Dirichlet kernel 
and a categorical log-likelihood over the first rolls of each frame of the $2010$ US Open (i.e. you can ignore the 
second rolls of each frame for now). You could use `for` loops, which can be specified in the Stan language as in R
```{stan output.var="for", eval = FALSE}
for (k in 1:K) {
  // do stuff, often indexing by k
}
```

## Running the Simplest Stan Program

Draw from the posterior distribution implied by simplest.stan . Remember that when you
pass the named list (or dict in Python) of data, add $1$ to both `roll_1` and `roll_2` so that they
correspond to categories rather than pins knocked down. In addition, you  will have to specify a vector
of $11$ positive numbers for `a` that imply a reasonable prior expectation of $\boldsymbol{\pi}$
among professional bowlers. Then, look at the resulting posterior summary.

## Simplest `generated quantities` Block

Below your `model` block, add a `generated quantities` block that evaluates the "posterior predictive
log probabilities" for the second roll in each frame of bowling, given the first roll in that frame. In a 
sense, the first roll is being treated as training data and the second roll is being treated as testing
data here. The steps involved are:

1. Declare a vector of size `N` to hold these predictive log probabilities
2. Execute a loop from `1` to `N` where you
    * Calculate the number of pins still upright for the second roll (and add $1$ to make it an index)
    * Subset the `pi` parameter vector so that its size is equal to this index,
      using either the `head` function or R-style indexing of a numeric vector by a sequance of integer 
      indices
    * Unless there was a gutter ball on the first roll, this subsetted vector cannot be interpreted
      as a set of conditional probabilities because it no longer adds up to $1$. Thus, renormalize it
      by dividing by its sum so that it can be interpreted as a set of conditional probabilities.
    * Evaluate the logarithm of the conditional probability of the second roll and stick it into
      the appropriate element of the vector you declared in step (1)

# Intermediate Model

Now we are going to extend the simplest model to consider all of the data and obtain posterior 
predictions for the score of a professional bowler over an entire game of ten frames.

## Intermediate `model` Block

Create a text file called intermediate.stan and paste the above `data` and `parameters` blocks into it. Below that,
write a `model` block that increments the `target` keyword with the logarithm of a Dirichlet kernel and 
a categorical log-likelihood over the first rolls of each frame and the second rolls of each frame (unless there
was a strike on the first roll, in which case the second roll does not happen). The log-likelihood contribution
of the second roll is very similar to the calculation in the `generated quantities` block of the previous
problem, although here we are conditioning on all the available data rather than separating off the second roll
as testing data.

## Intermediate `functions` and `generated quantities` Block

We can also draw from the posterior predictive distribution for a professional bowler's score for an
entire game of bowling, which consists of ten frames. The usual rules for scoring in bowling are given at

https://en.wikipedia.org/wiki/Ten-pin_bowling#Traditional_scoring

The main complication in the scoring rules is that if a bowler obtains a spare or a strike in this frame,
then bonus points are awarded depending on the result of _future_ rolls. Also, if a bowler obtains a spare 
or a strike in the tenth frame, an additional roll or two are awarded to accumulate bonus points.

To draw from a posterior predictive distribution of a bowler's score, you first need to simulate an
entire game of bowling, one roll at a time, using draws from the posterior distribution of $\boldsymbol{\pi}$.
For example, to draw from the posterior predictive distribution for the number of pins knocked down on the first 
roll of a frame of bowling, you could do

```{stan output.var="x_1", eval = FALSE}
  int x_1 = categorical_rng(pi) - 1;
```
so that `x_1` is a realization of pins knocked down between $0$ and $10$ (which would be a strike).

Edit intermediate.stan to add a `functions` block with a function called `scorecard_rng` that takes a
vector of probabilities that each number of pins gets knocked down on a roll and returns the bowler's
results for an entire simulated game in the form of a $12 \times 2$ integer array. The $12$ is for 
the possibility that two additional rolls are given for a strike in the tenth frame, and the $2$ is
for the number of possible rolls in each frame. Then, write another function called `score_game` that 
takes such a $12 \times 2$ integer array and calculates the bowler's score for the game. Here is an 
outline of those two functions:
```{stan output.var="scoring", eval = FALSE}
functions {
  int[,] scorecard_rng(vector pi) {
    int scorecard[12, 2] = rep_array(0, 12, 2); // initialize 12x2 array with zeros
    for (frame in 1:10) {
      // fill in scorecard with draws from the predictive distribution
    }
    if (scorecard[10, 1] == 10) { // strike in tenth frame
      // fill in rows 11 and / or 12 with draws from the predictive distribution
    } else if (sum(scorecard[10, ]) == 10) { // spare in tenth frame
      // fill in row 11 with draws from the predictive distribution
    }
    return scorecard;
  }
  
  int score_game(int[,] scorecard) {
    int score = 0;
    for (frame in 1:10) {
      // increment score based on results in scorecard
    }
    return score;
  }
}
```

Remember that inside a `for` loop, `if` clause, `else` clause, etc., you can declare new local
variables to keep your code organized and easy to read. However, these local variables have to
be declared right after the opening `{`. Thus, sometimes you have to declare a symbol but not
define it until farther down in your code. In particular, when simulating the second roll of a frame
of bowling, you will need to do something like

```{stan output.var="x_2", eval = FALSE}
if (x_1 < 10) { // not a strike
  int x_2; // declared now but defined later
  // more code to set up the conditional probabilities
  x_2 = categorical_rng(conditional_probabilies);
}
```

Finally, your `generated quantities` block should call the two functions that you 
defined in the `functions` block in order to store the posterior predictive distribution 
of the bowler's score in the output, like

```{stan output.var="score", eval = FALSE}
generated quantities {
  int score = score_game(scoreboard_rng(pi));
}
```

## Running the Intermediate Stan Program

Draw from the posterior distribution implied by intermediate.stan . How would you describe the 
resulting posterior predictive distribution of a professional bowler's score for a game of bowling?

# Multilevel Stan Program

To the extent that there is anything interesting about modeling bowling, it would be necessary
to allow bowlers to have different skill levels, as reflected in higher probabilities of knocking
down more of the available pins. 

In a multilevel model, each of the $J$ bowlers has a unique simplex of probabilities with (usually)
a Dirichlet prior that has the PDF

$$f\left(\boldsymbol{\pi}_j \mid \boldsymbol{\mu}, \gamma\right) = 
\frac{\Gamma\left(\gamma\right)}{\prod_{k = 1}^K \Gamma\left(\gamma \mu_k\right)} 
\prod_{k = 1}^K \pi_{j,k}^{\gamma \mu_k - 1}$$

where $\gamma \boldsymbol{\mu}$ is a vector of _unknown_ but positive shape parameters. $\gamma$
controls how much the distribution of $\boldsymbol{\pi}_j$ concentrates around 
$\boldsymbol{\mu} = \mathbb{E}\boldsymbol{\pi}_j$. A plausible prior distribution for $\gamma > 0$ 
is the exponential distribution, which has the PDF

$$f\left(\gamma \mid r\right) = r e^{-r \gamma},$$
where $r > 0$ is a rate hyperparameter, such that $\mathbb{E}\gamma = \frac{1}{r}$. Thus, larger
values of $r$ imply more heterogeneity in skill across professional bowlers under the prior. Since 
the unkown $\boldsymbol{\mu}$ is also a simplex of size $11$ that represents the probabilities of the
average professional bowler (in the $2010$ US Open bowling tournament) knocking down $0, 1, \dots, 10$
pins, you can use the same Dirichlet prior as in the simplest and intermediate Stan programs:

$$f\left(\boldsymbol{\mu} \mid \mathbf{a}\right) = \frac{\Gamma\left(\sum_{k = 1}^K a_k\right)}{\prod_{k = 1}^K \Gamma\left(a_k\right)} \prod_{k = 1}^K \mu_k^{a_k - 1}.$$
The contribution to the log-likelihood of the $j$-th bowler only involves $\boldsymbol{\pi}_j$, as
opposed to any other bowler's skill. Thus, we need to reformat the data to correspond to the restructured 
`data` block of the Stan program below. For simplicity, we only consider data on the first round of the $2010$ 
US Open bowling tournament:
```{r}
# drop subsequent rounds
US_Open2010 <- US_Open2010[!duplicated(paste0(US_Open2010$Bowler, "_", US_Open2010$Frame)), ]
J <- length(unique(US_Open2010$Bowler)) # number of bowlers
roll_1 <- t(matrix(US_Open2010$roll_1 + 1, nrow = 10, ncol = J))
roll_2 <- t(matrix(US_Open2010$roll_2 + 1, nrow = 10, ncol = J))
```

## Multilevel Stan Program

Create a text file called multilevel.stan and paste the following into it.
```{stan output.var="multilevel", eval = FALSE}
functions {
  // copy your scoreboard_rng and score_game functions here
}
data {
  int<lower = 0> J;                         // number of bowlers
  int<lower = 1, upper = 11> roll_1[J, 10]; // 1 = gutter ball, ..., 11 = strike
  int<lower = 1, upper = 11> roll_2[J, 10]; // same
  vector<lower = 0>[11] a; // hyperparameters for Dirichlet prior on mu
  real<lower = 0> rate;    // hyperparameter for exponential prior on gamma
}
parameters {
  simplex[11] pi[J];       // probability of 1 ... 11 for each bowler
  simplex[11] mu;          // expectation of pi_j across professional bowlers
  real<lower = 0> gamma;   // concentration of professional bowlers around mu
}
model {
  // write this code to accumulate the log-kernel in target
}
generated quantities {
  int score[J];
  int winner;
  for (j in 1:J) score[j] = game_rng(pi[j]);
  winner = sort_indices_desc(score)[1]; // index of bowler with highest score
}
```
In the `generated quantities` block, in addition to simulating the score for
each of the $J$ bowlers, we also compute who has the maximum simulated score.
But first, you need to fill in the model block so that it accumulates the
log-kernel of the posterior distribution in the `target` keyword.

## Running the Multilevel Stan Program

Draw from the posterior distribution implied by multilevel.stan . `J`, `roll_1`, and `roll_2` 
can be passed as part of the named list (in R) or dict (in Python), but be sure to also specify $r$.
You can pull out the posterior draws on `winner` or any other quantity in R
by doing
```{r, eval = FALSE}
winner <- rstan::extract(multilevel)[[1]]
```
which is a vector of size $4000$ of the bowler with the highest score. Based on
```{r, eval = FALSE}
prop.table(table(winner))
```
who has the highest probability of winning the bowling tournament?
